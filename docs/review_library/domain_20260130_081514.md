# Domain Correctness & Compliance Review - KMI Manager CLI
Date: 2026-01-30

## Reviewer Profile
I bring 30 years of experience building and operating API platforms, key custody systems, and developer tooling in regulated environments. I have designed key rotation and rate limiting policies that satisfy both security objectives and provider ToS constraints, and I have led audit-readiness efforts where logs and provenance mattered as much as uptime. Over the years I have reviewed incident retrospectives where small defaults caused compliance drift, so I focus on whether guardrails are explicit and reversible. This review applies that lens to key pooling, auditability, and operational safety.

## Scope and Evidence
- Key rotation logic: `src/kmi_manager_cli/rotation.py`
- Proxy routing and auth: `src/kmi_manager_cli/proxy.py`
- Health and usage scoring: `src/kmi_manager_cli/health.py`
- Key registry and account loading: `src/kmi_manager_cli/keys.py`, `src/kmi_manager_cli/auth_accounts.py`
- Config and policy toggles: `src/kmi_manager_cli/config.py`, `src/kmi_manager_cli/cli.py`, `src/kmi_manager_cli/errors.py`
- State, logs, and trace: `src/kmi_manager_cli/state.py`, `src/kmi_manager_cli/trace.py`, `src/kmi_manager_cli/logging.py`, `src/kmi_manager_cli/ui.py`
- Requirements and compliance intent: `docs/sdd/kmi-rotation-sdd/requirements.md`

## Executive Summary
The system meets the stated functional requirements for local-first rotation and proxying, including opt-in remote binding, optional rate limiting, and clear ToS reminders. The biggest domain risks are weak policy boundaries for key pooling, the lack of a clean auto-rotation disable workflow once enabled, and limited auditability (no actor attribution, retention policy, or integrity controls). Operational safety is good for single-operator local use, but multi-user or shared hosts can expose keys and logs. Addressing these gaps will materially reduce compliance exposure and improve safe operation under real-world load.

## Findings

### API Key Management and Rotation
- Keys are loaded from `_auths/` or `~/.kimi/_auths` across `.env`, `.toml`, `.json`, and `.bak` formats and deduplicated by raw API key. There is no policy boundary to ensure keys belong to a single org/tenant or even the same provider base URL, so pooling across unrelated accounts is possible without warning. `src/kmi_manager_cli/keys.py`, `src/kmi_manager_cli/auth_accounts.py`.
- Auto-rotation is gated by `KMI_AUTO_ROTATE_ALLOWED` but defaults to allowed, and once enabled it persists in `~/.kmi/state.json`. There is no CLI command to disable auto-rotation or clear the persisted state. `src/kmi_manager_cli/config.py`, `src/kmi_manager_cli/cli.py`, `src/kmi_manager_cli/state.py`.
- Proxy logic treats 401 as a permanent block, while 403/429 trigger a cooldown (default 300s) and the key becomes eligible again after cooldown. Repeated 403s are not escalated to permanent block, which can lead to repeated attempts on a key that is actually banned or policy-restricted. `src/kmi_manager_cli/proxy.py`, `src/kmi_manager_cli/rotation.py`, `src/kmi_manager_cli/state.py`.
- Manual rotation relies on usage-derived health scoring. In `KMI_DRY_RUN=1`, usage is mocked as 100% remaining, so health-based decisions are not validated against real quotas. This is safe for testing but can mislead operational expectations if not clearly understood. `src/kmi_manager_cli/health.py`, `src/kmi_manager_cli/cli.py`.
- The tool performs request-level load balancing, not lifecycle key rotation (issue/revoke/retire). If security policy expects periodic key re-issuance or revocation workflows, that is out of scope and not automated here. `src/kmi_manager_cli/rotation.py`, `src/kmi_manager_cli/cli.py`.

### Auditability and Data Handling
- Logs and trace are JSON/JSONL under `~/.kmi/` with rotation, but there is no retention policy, integrity protection, or tamper-evidence. Entries include key labels, key hashes, endpoints, status, and latency but omit client identity or actor attribution, which limits forensic usefulness. `src/kmi_manager_cli/logging.py`, `src/kmi_manager_cli/trace.py`, `src/kmi_manager_cli/proxy.py`.
- The UI can display account emails derived from auth files or usage API responses. Emails are not redacted, so operators can inadvertently expose PII on shared terminals or screenshots. `src/kmi_manager_cli/health.py`, `src/kmi_manager_cli/ui.py`.
- State keeps counters and timestamps but does not capture who enabled auto-rotation, when configuration was rewritten, or why a key was disabled. This limits audit trails for policy enforcement. `src/kmi_manager_cli/state.py`, `src/kmi_manager_cli/cli.py`.

### Operational Safety and Access Control
- Remote proxy binding is an explicit opt-in and requires a token, which is a good guardrail. However, local binding is unauthenticated and can be accessed by any local user on a shared host. `src/kmi_manager_cli/proxy.py`.
- Rate limiting is proxy-wide, not per-key. With multiple clients, traffic can exceed per-key quotas even with round-robin rotation, risking provider SLA/ToS violations. `src/kmi_manager_cli/proxy.py`.
- State is loaded once at startup and then mutated in memory. Running multiple proxy processes can lead to stale or conflicting state (rotation index, counters), resulting in uneven distribution and harder auditability. `src/kmi_manager_cli/state.py`, `src/kmi_manager_cli/proxy.py`.
- Manual rotation can overwrite `~/.kimi/config.toml` (default `KMI_WRITE_CONFIG=1`) without backup or audit. On shared machines, this can unintentionally change the active account for other tools. `src/kmi_manager_cli/cli.py`, `src/kmi_manager_cli/auth_accounts.py`.

### Compliance and Policy Alignment
- Requirements explicitly call for disabling auto-rotation when pooling is prohibited. The policy flag exists, but there is no explicit disable command and the default allows auto-rotation, which can drift from intended compliance posture. `docs/sdd/kmi-rotation-sdd/requirements.md`, `src/kmi_manager_cli/config.py`, `src/kmi_manager_cli/cli.py`.
- The tool forwards requests to upstream and strips proxy auth headers, which avoids leaking proxy tokens upstream. This is a positive compliance measure. `src/kmi_manager_cli/proxy.py`.
- There are no permission checks or warnings if key files, state, or logs are world-readable. In regulated environments, this can violate internal controls for secret material or PII. `src/kmi_manager_cli/keys.py`, `src/kmi_manager_cli/state.py`, `src/kmi_manager_cli/logging.py`.

## Recommendations (Prioritized)
1. Add an explicit disable command (`kmi rotate off` or similar) that clears `state.auto_rotate` and logs the action; require a fresh opt-in to re-enable.
2. Introduce policy boundaries for key pooling: allowlist base URLs or account emails, and optionally enforce single-org pools unless explicitly overridden.
3. Improve 403 handling by distinguishing permanent auth failures vs temporary policy/rate errors; consider escalating repeated 403s to blocked state or requiring operator acknowledgment.
4. Add per-key rate limiting or concurrency caps to better respect provider quotas under multi-client load.
5. Add optional audit metadata (client IP, operator identity, action logs) and document a retention and purge strategy for logs and trace files.
6. Perform file permission checks on `_auths` and `~/.kmi` artifacts; warn when files are group/world-readable.
7. Consider defaulting `KMI_WRITE_CONFIG=0` and requiring explicit opt-in to mutate `~/.kimi/config.toml`, or write a backup with timestamp.
8. Document the limits of dry-run health scoring and clearly separate testing vs production behavior in the CLI help.

## Open Questions
- Should 403 responses be treated as permanent blocks for this provider, or are they often temporary policy or quota failures that warrant only cooldown?
- Is key pooling across multiple accounts an intended feature, and if so, is there a requirement to verify common ownership (org ID or email match)?
- Do you need a formal audit trail or retention policy for `~/.kmi` logs and trace data to satisfy internal compliance requirements?
- Should local proxy access be considered trusted, or do you need an auth token even for localhost in shared environments?
